"""Mesop event handlers â€” on_load, uploads, generation flows, cancel.

All async generators that drive the progress UI live here.

v2.1 â€” Async optimisations:
  â€¢ Shared ``ThreadPoolExecutor`` (bounded, reused across requests)
  â€¢ Per-request ``CancelToken`` (safe under concurrent users)
  â€¢ ``run_in_executor`` helper for clean ``await`` syntax
  â€¢ PDF/PPTX rendering offloaded to thread pool
"""

from __future__ import annotations

import asyncio
import base64
import contextlib
import logging
import os
import re
import tempfile

import mesop as me

from app.config import settings
from app.core.cancellation import CancelToken, clear_cancel_signal, set_cancel_signal
from app.core.executor import get_executor, run_in_executor
from app.core.log import safe_print
from app.providers.ollama import OllamaProvider
from app.rendering.pdf import save_summary_to_pdf
from app.rendering.pptx import create_pptx
from app.services.review import PartialCompletionError, review_book_syntopic
from app.services.slide import analyze_document
from app.services.summary import summarize_book_deep_dive, summarize_document
from app.ui.state import State

# â”€â”€ Lifecycle â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def on_load(e: me.LoadEvent) -> None:
    me.set_theme_mode("system")
    state = me.state(State)
    state.error_message = ""
    state.processing_status = "idle"
    if state.logs is None:
        state.logs = []
    state.logs.append("Há»‡ thá»‘ng Ä‘Ã£ khá»Ÿi Ä‘á»™ng. Sáºµn sÃ ng xá»­ lÃ½.")

    # Auto-detect provider
    if not state.ai_provider:
        ollama_url = os.environ.get("OLLAMA_BASE_URL", "")
        if ollama_url:
            ollama = OllamaProvider(base_url=ollama_url)
            if ollama.check_connectivity():
                state.ai_provider = "ollama"
                state.logs.append("ğŸŸ¢ Auto-detected Ollama server. Sá»­ dá»¥ng Local LLM (miá»…n phÃ­).")
            elif os.environ.get("GOOGLE_API_KEY"):
                state.ai_provider = "gemini"
            else:
                state.ai_provider = "ollama"
                state.logs.append("âš ï¸ Ollama server khÃ´ng pháº£n há»“i. Kiá»ƒm tra server Ä‘ang cháº¡y chÆ°a.")
        elif os.environ.get("GOOGLE_API_KEY"):
            state.ai_provider = "gemini"
            state.logs.append("ğŸ”‘ Sá»­ dá»¥ng Google Gemini API.")
        elif os.environ.get("OPENAI_API_KEY"):
            state.ai_provider = "openai"
            state.logs.append("ğŸ”‘ Sá»­ dá»¥ng OpenAI API.")
        else:
            state.ai_provider = "ollama"
            state.logs.append("âš ï¸ KhÃ´ng tÃ¬m tháº¥y API Key. Máº·c Ä‘á»‹nh dÃ¹ng Ollama (Local LLM).")


# â”€â”€ Simple input handlers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def handle_upload(event: me.UploadEvent) -> None:
    state = me.state(State)
    file = event.file
    file_bytes = file.read()
    size_mb = len(file_bytes) / (1024 * 1024)
    if size_mb > settings.max_upload_size_mb:
        state.error_message = f"File quÃ¡ lá»›n ({size_mb:.1f} MB). Giá»›i háº¡n: {settings.max_upload_size_mb} MB."
        return
    state.uploaded_file_bytes = file_bytes
    state.uploaded_mime_type = file.mime_type
    state.uploaded_filename = file.name
    state.logs = [f"ÄÃ£ táº£i lÃªn: {file.name}", "System: Console Output Suppressed (v3)"]
    state.processing_status = "ready"
    state.error_message = ""


def handle_topic_input(e: me.InputEvent) -> None:
    me.state(State).user_topic = e.value


def handle_template_upload(event: me.UploadEvent) -> None:
    state = me.state(State)
    state.template_file_bytes = event.file.read()
    state.template_filename = event.file.name
    state.logs.append(f"ÄÃ£ táº£i lÃªn máº«u: {event.file.name}")


def on_detail_change(e: me.CheckboxChangeEvent) -> None:
    me.state(State).is_detailed = e.checked


def on_multi_key_change(e: me.CheckboxChangeEvent) -> None:
    me.state(State).use_multi_key = e.checked


def on_language_change(e: me.SelectSelectionChangeEvent) -> None:
    me.state(State).review_language = e.value


def on_provider_change(e: me.SelectSelectionChangeEvent) -> None:
    me.state(State).ai_provider = e.value


def handle_openai_keys_input(e: me.InputEvent) -> None:
    me.state(State).openai_api_keys_input = e.value


def handle_ollama_url_input(e: me.InputEvent) -> None:
    me.state(State).ollama_base_url = e.value


def handle_api_keys_input(e: me.InputEvent) -> None:
    me.state(State).user_api_keys_input = e.value


def handle_user_instruction(e: me.InputEvent) -> None:
    me.state(State).user_instructions = e.value


def set_topic(e: me.ClickEvent) -> None:
    me.state(State).user_topic = e.key


# â”€â”€ Cancel flow â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


async def request_cancel(e: me.ClickEvent):
    state = me.state(State)
    state.show_cancel_dialog = True
    yield


def dismiss_cancel(e: me.ClickEvent) -> None:
    me.state(State).show_cancel_dialog = False


# â”€â”€ Active cancel token (per-request, set from generation flows) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_active_token: CancelToken | None = None


def confirm_cancel(e: me.ClickEvent) -> None:
    state = me.state(State)
    state.show_cancel_dialog = False
    state.cancel_requested = True
    set_cancel_signal()
    if _active_token is not None:
        _active_token.cancel()
    state.logs.append("âš ï¸ Äang yÃªu cáº§u há»§y bá»... Vui lÃ²ng Ä‘á»£i bÆ°á»›c hiá»‡n táº¡i hoÃ n táº¥t.")


# â”€â”€ Key resolution helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _resolve_api_keys(state: State) -> tuple[list[str], str]:
    """Return ``(api_keys_list, provider_name)`` derived from state."""
    provider = state.ai_provider or "gemini"

    if provider == "openai":
        keys: list[str] = []
        if state.openai_api_keys_input:
            keys = [k.strip() for k in re.split(r"[,\n\r]+", state.openai_api_keys_input) if k.strip()]
        env = os.environ.get("OPENAI_API_KEY")
        if env and env not in keys:
            keys.append(env)
        return keys, provider

    if provider == "ollama":
        base_url = state.ollama_base_url or settings.ollama_base_url
        return [base_url], provider

    # Gemini
    keys = []
    env = os.environ.get("GOOGLE_API_KEY")
    if state.use_multi_key and state.user_api_keys_input:
        keys = [k.strip() for k in re.split(r"[,\n\r]+", state.user_api_keys_input) if k.strip()]
    if env and env not in keys:
        keys.append(env)
    return keys, provider


def _generate_pdf_and_store(state: State, data: dict, suffix: str) -> None:
    """Render PDF, encode to base64, store on *state*.

    Called from within the thread pool, so all I/O is non-blocking to the UI.
    """
    name_no_ext = state.uploaded_filename.rsplit(".", 1)[0]
    safe_name = re.sub(r"[^\w\s\-.]", "", name_no_ext)
    pdf_out_name = f"{safe_name}_{suffix}.pdf"

    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp_path = tmp.name

    final_path = save_summary_to_pdf(data, tmp_path)
    with open(final_path, "rb") as f:
        state.pdf_content_base64 = base64.b64encode(f.read()).decode("utf-8")
    state.pdf_filename = pdf_out_name
    with contextlib.suppress(Exception):
        os.remove(final_path)
    state.logs.append(f"ÄÃ£ táº¡o xong file: {state.pdf_filename}")


async def _poll_future(future, token: CancelToken, state: State):
    """Poll a future with cancel checks, yielding for Mesop UI updates.

    Returns the future result, or None if cancelled.
    """
    while not future.done():
        if token.is_set() or me.state(State).cancel_requested:
            state.processing_status = "idle"
            state.logs.append("âŒ ÄÃ£ há»§y bá» lá»‡nh.")
            future.cancel()
            yield None
            return
        yield  # let Mesop re-render
        await asyncio.sleep(0.3)
    yield future.result()


# â”€â”€ Async generation flows â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


async def generate_summary(e: me.ClickEvent):
    global _active_token
    state = me.state(State)
    state.error_message = ""
    state.cancel_requested = False
    token = CancelToken()
    _active_token = token
    clear_cancel_signal()
    yield

    if not state.uploaded_file_bytes:
        state.error_message = "Vui lÃ²ng táº£i lÃªn file tÃ i liá»‡u trÆ°á»›c."
        yield
        return

    state.processing_status = "analyzing_summary"
    api_keys_list, provider = _resolve_api_keys(state)
    label = {"openai": "OpenAI", "ollama": "Ollama (Local)"}.get(provider, "Gemini")
    state.logs.append(f"Source: {state.uploaded_filename} | Provider: {label}")
    state.logs.append(f"Äang tÃ³m táº¯t tÃ i liá»‡u vá»›i {label}...")
    yield

    if token.is_set():
        state.processing_status = "idle"
        state.logs.append("âŒ ÄÃ£ há»§y bá» lá»‡nh.")
        yield
        return

    try:
        executor = get_executor()
        if state.is_detailed:
            state.logs.append("Äang cháº¡y cháº¿ Ä‘á»™ Deep Dive...")
            yield
            future = executor.submit(
                summarize_book_deep_dive,
                state.uploaded_file_bytes,
                state.uploaded_mime_type,
                api_keys=api_keys_list,
                cancel_check=token.is_set,
                provider=provider,
            )
        else:
            future = executor.submit(
                summarize_document,
                state.uploaded_file_bytes,
                state.uploaded_mime_type,
                api_keys=api_keys_list,
                user_instructions=state.user_instructions,
                cancel_check=token.is_set,
                provider=provider,
            )

        while not future.done():
            if token.is_set() or me.state(State).cancel_requested:
                state.processing_status = "idle"
                state.logs.append("âŒ ÄÃ£ há»§y bá» lá»‡nh.")
                future.cancel()
                yield
                return
            yield
            await asyncio.sleep(0.3)

        summary_data = future.result()

        if not summary_data:
            raise Exception("Empty result from executor")

        if "used_model" in summary_data:
            state.logs.append(f"Model used: {summary_data['used_model']}")

        state.logs.append("TÃ³m táº¯t hoÃ n táº¥t. Äang táº¡o PDF...")
        state.processing_status = "generating_pdf"
        yield

        if token.is_set():
            state.processing_status = "idle"
            state.logs.append("âŒ ÄÃ£ há»§y bá» lá»‡nh.")
            yield
            return

        # Offload PDF rendering to thread pool
        await run_in_executor(_generate_pdf_and_store, state, summary_data, "summary")
        state.processing_status = "summary_done"
        yield
    except Exception as ex:
        safe_print(f"MAIN EXCEPTION: {ex}", logging.ERROR)
        state.processing_status = "error"
        state.error_message = str(ex)
        state.logs.append(f"Lá»—i: {ex}")
        yield
    finally:
        _active_token = None


async def generate_slides(e: me.ClickEvent):
    global _active_token
    state = me.state(State)
    state.error_message = ""
    state.cancel_requested = False
    token = CancelToken()
    _active_token = token
    clear_cancel_signal()
    yield

    if not state.uploaded_file_bytes:
        state.error_message = "Vui lÃ²ng táº£i lÃªn file tÃ i liá»‡u trÆ°á»›c."
        yield
        return

    state.processing_status = "analyzing"
    api_keys_list, provider = _resolve_api_keys(state)
    label = {"openai": "OpenAI", "ollama": "Ollama (Local)"}.get(provider, "Gemini")
    state.logs.append(f"Source: {state.uploaded_filename} | Provider: {label}")
    if state.template_filename:
        state.logs.append(f"Template: {state.template_filename}")
    detail_mode = "Chi tiáº¿t" if state.is_detailed else "TÃ³m táº¯t"
    state.logs.append(f"Äang phÃ¢n tÃ­ch tÃ i liá»‡u ({detail_mode})...")
    yield

    if token.is_set():
        state.processing_status = "idle"
        state.logs.append("âŒ ÄÃ£ há»§y bá» lá»‡nh.")
        yield
        return

    try:
        executor = get_executor()
        future = executor.submit(
            analyze_document,
            state.uploaded_file_bytes,
            state.uploaded_mime_type,
            api_keys=api_keys_list,
            detail_level=detail_mode,
            user_instructions=state.user_instructions,
            cancel_check=token.is_set,
            provider=provider,
        )

        while not future.done():
            if token.is_set() or me.state(State).cancel_requested:
                state.processing_status = "idle"
                state.logs.append("âŒ ÄÃ£ há»§y bá» lá»‡nh.")
                future.cancel()
                yield
                return
            yield
            await asyncio.sleep(0.3)

        slide_json = future.result()

        if not slide_json:
            raise Exception("AI khÃ´ng tráº£ vá» dá»¯ liá»‡u slide.")

        state.logs.append("PhÃ¢n tÃ­ch hoÃ n táº¥t. Äang táº¡o slide...")
        state.processing_status = "generating"
        yield

        if token.is_set():
            state.processing_status = "idle"
            state.logs.append("âŒ ÄÃ£ há»§y bá» lá»‡nh.")
            yield
            return

        # Offload PPTX rendering to thread pool
        pptx_io = await run_in_executor(
            create_pptx,
            slide_json,
            template_pptx_bytes=state.template_file_bytes if state.template_file_bytes else None,
        )
        pptx_bytes = pptx_io.read()
        name_no_ext = state.uploaded_filename.rsplit(".", 1)[0]
        safe_name = re.sub(r"[^\w\s\-.]", "", name_no_ext)
        state.pptx_filename = f"{safe_name}_presentation.pptx"
        state.pptx_content_base64 = base64.b64encode(pptx_bytes).decode("utf-8")
        state.logs.append(f"ÄÃ£ táº¡o xong file: {state.pptx_filename}")
        state.processing_status = "done"
        yield
    except Exception as ex:
        safe_print(f"MAIN EXCEPTION: {ex}", logging.ERROR)
        state.processing_status = "error"
        state.error_message = str(ex)
        state.logs.append(f"Lá»—i: {ex}")
        yield
    finally:
        _active_token = None


async def generate_review(e: me.ClickEvent):
    global _active_token
    state = me.state(State)
    state.error_message = ""
    state.cancel_requested = False
    state.resume_data = {}
    token = CancelToken()
    _active_token = token
    clear_cancel_signal()
    yield

    if not state.uploaded_file_bytes:
        state.error_message = "Vui lÃ²ng táº£i lÃªn file tÃ i liá»‡u trÆ°á»›c."
        yield
        return

    state.processing_status = "analyzing_review"
    api_keys_list, provider = _resolve_api_keys(state)
    label = {"openai": "OpenAI", "ollama": "Ollama (Local)"}.get(provider, "Gemini")
    state.logs.append(f"Source: {state.uploaded_filename} | Provider: {label}")
    state.logs.append("Äang cháº¡y Syntopic Book Review (3 Agents)...")
    yield

    if token.is_set():
        state.processing_status = "idle"
        state.logs.append("âŒ ÄÃ£ há»§y bá» lá»‡nh.")
        yield
        return

    try:
        executor = get_executor()
        future = executor.submit(
            review_book_syntopic,
            state.uploaded_file_bytes,
            state.uploaded_mime_type,
            api_keys=api_keys_list,
            language=state.review_language,
            cancel_check=token.is_set,
            provider=provider,
        )

        while not future.done():
            if token.is_set() or me.state(State).cancel_requested:
                state.processing_status = "idle"
                state.logs.append("âŒ ÄÃ£ há»§y bá» lá»‡nh.")
                future.cancel()
                yield
                return
            yield
            await asyncio.sleep(0.3)

        review_data = future.result()

        if "used_model" in review_data:
            state.logs.append(f"Model used: {review_data['used_model']}")

        state.logs.append("Review hoÃ n táº¥t. Äang táº¡o PDF...")
        state.processing_status = "generating_pdf"
        yield

        if token.is_set():
            state.processing_status = "idle"
            yield
            return

        # Offload PDF rendering to thread pool
        await run_in_executor(_generate_pdf_and_store, state, review_data, "expert_review")
        state.processing_status = "review_done"
        yield

    except PartialCompletionError as partial_ex:
        safe_print(f"PARTIAL ERROR: {partial_ex}", logging.WARNING)
        state.processing_status = "error"
        state.error_message = f"{partial_ex} (CÃ³ thá»ƒ tiáº¿p tá»¥c)"
        state.resume_data = partial_ex.partial_data
        state.logs.append(f"âš ï¸ Lá»—i má»™t pháº§n: {partial_ex}. Dá»¯ liá»‡u Ä‘Ã£ lÆ°u Ä‘á»ƒ tiáº¿p tá»¥c.")
        yield
    except Exception as ex:
        safe_print(f"MAIN EXCEPTION: {ex}", logging.ERROR)
        state.processing_status = "error"
        state.error_message = str(ex)
        state.logs.append(f"Lá»—i Review: {ex}")
        yield
    finally:
        _active_token = None


async def resume_review(e: me.ClickEvent):
    global _active_token
    state = me.state(State)
    if not state.resume_data:
        state.error_message = "KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ tiáº¿p tá»¥c."
        yield
        return

    state.error_message = ""
    state.cancel_requested = False
    token = CancelToken()
    _active_token = token
    clear_cancel_signal()
    yield

    state.processing_status = "analyzing_review"
    state.logs.append("ğŸ”„ Äang tiáº¿p tá»¥c xá»­ lÃ½ (Resume)...")
    yield

    if token.is_set():
        state.processing_status = "idle"
        state.logs.append("âŒ ÄÃ£ há»§y bá» lá»‡nh.")
        yield
        return

    try:
        api_keys_list, provider = _resolve_api_keys(state)
        executor = get_executor()
        future = executor.submit(
            review_book_syntopic,
            state.uploaded_file_bytes,
            state.uploaded_mime_type,
            api_keys=api_keys_list,
            language=state.review_language,
            cancel_check=token.is_set,
            resume_state=state.resume_data,
            provider=provider,
        )

        while not future.done():
            if token.is_set() or me.state(State).cancel_requested:
                state.processing_status = "idle"
                state.logs.append("âŒ ÄÃ£ há»§y bá» lá»‡nh.")
                future.cancel()
                yield
                return
            yield
            await asyncio.sleep(0.3)

        review_data = future.result()

        if "used_model" in review_data:
            state.logs.append(f"Model used: {review_data['used_model']}")

        state.logs.append("Review hoÃ n táº¥t. Äang táº¡o PDF...")
        state.processing_status = "generating_pdf"
        state.resume_data = {}
        yield

        if token.is_set():
            state.processing_status = "idle"
            yield
            return

        await run_in_executor(_generate_pdf_and_store, state, review_data, "expert_review")
        state.processing_status = "review_done"
        yield

    except PartialCompletionError as partial_ex:
        safe_print(f"PARTIAL ERROR (RESUME): {partial_ex}", logging.WARNING)
        state.processing_status = "error"
        state.error_message = f"{partial_ex} (CÃ³ thá»ƒ tiáº¿p tá»¥c)"
        state.resume_data = partial_ex.partial_data
        state.logs.append(f"âš ï¸ Láº¡i gáº·p lá»—i: {partial_ex}. ÄÃ£ cáº­p nháº­t Ä‘iá»ƒm dá»«ng.")
        yield
    except Exception as ex:
        safe_print(f"MAIN EXCEPTION: {ex}", logging.ERROR)
        state.processing_status = "error"
        state.error_message = str(ex)
        state.logs.append(f"Lá»—i Review: {ex}")
        yield
    finally:
        _active_token = None
